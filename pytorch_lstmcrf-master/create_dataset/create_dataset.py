# -*- coding: utf-8 -*-
"""create_dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SPmhNleS6rwBiRL6MTHryg_lMWI5dDUJ
"""

#!mkdir -p data_09-12-2019/one/
#!python -m spacy download en
#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz

# from google.colab import drive
# drive.mount('/content/drive')

import json
import re
import os
import io
import spacy
import argparse
from tqdm import tqdm
import en_core_sci_sm
import unicodedata
import random

# Commented out IPython magic to ensure Python compatibility.
# %reset

class CreateDataset:
    def __init__(self, add_id, add_section, all_lines):

        self.single_annotation = re.compile(r'(.*?)<<<<(.*?)>>>>##\[(.*?)\](.*?)')
        self.sequence_annotation_start = re.compile(r'(.*?)<<<<(.*)')
        self.sequence_annotation_end = re.compile(r'(.*?)>>>>##\[(.*?)\](.*?)')
        
        self.replace = re.compile(r'<<<<(.*?)>>>>##\[(.*?)\]')

        self.SUBSTITUTIONS = {
            u'ﬀ': 'ff',
            u'ﬁ': 'fi',
            u'ﬂ': 'fl',
            u'“': "``",
            u'”': "''",
            u'⫺': "-",
            u'−' : "-",
            u"…" : "...",
            u"⫽" : "=",
            u'~' : "(",
            u"!" : ")",
            u'共' : "(",
            u'兲' : ')',
            u'' : '-',
            u'' : '',
            u'': " ",
            u'¼' : '=',
            u'1⁄4' : '=',
            u';' : ' ; ',
            u'.' : ' . ',
            u':' : ' : ',
            u',' : ' , ',
            u')' : ' ) ',
            u'(' : ' ( ',
            u'-' : ' - ',
            u'\\' : ' \\ '
        }


        # self.braces = re.compile(r'~(.*?)!')
        # self.charsub_empty = re.compile(r"(['\\\-\(\);])")
        # self.charsub_space = re.compile(r"([\.:,])")
        # self.hyphen = re.compile(r"−")
        # self.equals = re.compile(r"¼")
        # self.equals2 = re.compile(r"1⁄4")
        
        self.mapper = {'': 'INVALID', 'result': 'RESULT', 'method':'METHOD',
'parameters':'PARAMETER', 'parameter':'PARAMETER','parameters l':'PARAMETER', 'material':'MATERIAL',
'meterial':'MATERIAL', 'materials':'MATERIAL', 'xc':'METHOD', 'code':'CODE', 'structure':'STRUCTURE'}

        self.add_id = add_id
        self.add_section = add_section
        self.all_lines = all_lines

        self.nlp = en_core_sci_sm.load()


    def compute(self, tokens, labels):
        in_between = False
        selected_portion = []
        output = []

        #Iterate through all the tokens in current section
        for token in tokens:
            #Some labels have \x00 stray character. Removing it is the current startegy
            token = token.replace(u'\x00','')
            if token.strip() == "":
                continue

            #If the token is part of a tagged section
            if in_between:

                #If the token is the end of the tagged section
                matched_obj = self.sequence_annotation_end.search(token)
                if matched_obj:
                    #This is the end

                    before_angular = matched_obj.group(1).strip()
                    after_label = matched_obj.group(3).strip()

                    annotated_text = before_angular + after_label
                    label = matched_obj.group(2)

                    #first add the first word with the label and B prefix
                    #then add the remainig words with the label and I prefix
                    output.append((selected_portion[0],'B-'+self.mapper[label.lower()]))
                    for token in selected_portion[1:]:
                        output.append((token,'I-'+self.mapper[label.lower()]))


                    highlighted = ' '.join(selected_portion)

                    #deal with the last character
                    #if there is a token before angular then we add it with right side 
                    #if there is no token before angular but token after the label
                    #then add the token after label with O as label
                    if before_angular != '':
                        output.append((annotated_text,'I-'+self.mapper[label.lower()]))
                        highlighted += ' '+annotated_text
                    elif before_angular=='' and after_label!='':
                        output.append((after_label,' O'))


                    #add the highlighted portion and annotation to the list of labels
                    if label != '' and highlighted!='':
                        labels.append([highlighted, self.mapper[label.lower()]])

                    #clear the buffers and initiate the new tokens
                    in_between = False
                    selected_portion = []
                else:
                    #This is in_between
                    selected_portion.append(token)

            else:
                #If the token is a single labelled section
                matched_obj = self.single_annotation.search(token) 
                if matched_obj:

                    before_angular = matched_obj.group(1).strip()
                    between_angular = matched_obj.group(2).strip()
                    after_label = matched_obj.group(4).strip()
        
                    annotated_text = before_angular + between_angular + after_label
                    label = matched_obj.group(3)

                    #this is a single token annotation. so we can just add it into the list

                    if label!='' and annotated_text!='':
                      labels.append([annotated_text, self.mapper[label.lower()]])
                      output.append((annotated_text,'B-'+self.mapper[label.lower()]));

                else:
                    #If this if the start of a labelled sequence
                    matched_obj = self.sequence_annotation_start.search(token)

                    if matched_obj:
                        #This is the start

                        before_angular = matched_obj.group(1).strip()
                        after_angular = matched_obj.group(2).strip()

                        #this is a start of a sequence of annotated tokens
                        #so we add it ti the list and toggle the switch
                        in_between = True
                        selected_portion.append(before_angular + after_angular)
                        # print(selected_portion)
                    else:
                        #No tags
                        output.append((token,'O'))

        return output

    #Add the lines in this section to lines section.
    def process(self, res, lines, heading, path):
        line = []
        if self.add_section:
            line.append(['section',str(heading)])
        if self.add_id:
            line.append(['id',str(path)])
            
        #the second part is going to be updated later anyways
        #so the label that is put here doesnt matter. Put O anyways
        for each in res:
            line.append([each[0], 'O'])

        if len(res) > 0:
            lines.append(line)

    def rectify(self, sentence):
        # sentence = (self.braces).sub(r'\1', sentence);
        # sentence = (self.charsub_empty).sub(r' \1 ', sentence);
        # sentence = (self.charsub_space).sub(r' \1 ', sentence);
        # sentence = (self.hyphen).sub(r' - ', sentence);
        # sentence = (self.equals).sub(r'=', sentence);
        # sentence = (self.equals2).sub(r'=', sentence);
        sentence = unicodedata.normalize('NFC', sentence)
        sentence = ''.join([self.SUBSTITUTIONS.get(c, c) for c in sentence])
        return sentence

    def addSection(self, section, text, labels, lines, path):
        try:
            if text:
                if section == 'TITLE':
                    sentences = [text]
                else:
                    sentences = self.nlp(text).sents
                for sentence in sentences:
                    if section != 'TITLE':
                        sentence = str(sentence.string)
                    sentence = self.rectify(sentence)

                    output = self.compute(sentence.split(), labels)
                    self.process(output, lines, section, path)
        except UnicodeEncodeError:
            pass

    def processFile(self, paper, labels, lines):
        #load the paper as json object
        with open(paper) as f:
            try:
                data = json.load(f)
            except ValueError:
                return

            # print(paper)
            metadata = data['metadata'] if 'metadata' in data else data
            title = metadata['title'] if 'title' in metadata and metadata['title'] else 'None'
            abstractText = metadata['abstractText']  if 'abstractText' in metadata and metadata['abstractText'] else ''

            #add the sections
            self.addSection('TITLE', title, labels, lines, paper)
            self.addSection('ABSTRACT', abstractText, labels, lines, paper)

            sections = metadata['sections'] if 'sections' in metadata else []
            if sections:
                for i,section in enumerate(sections):
                    text = section['text']
                    heading = section['heading'] if 'heading' in section and section['heading'] is not None else 'None'
                    if heading.lower() in ['acknowledgement', 'acknowledgements']:
                        continue
                    heading = str(i)
                    # print(heading)
                    self.addSection(heading, text, labels, lines, paper)

        #soring the tokens in decreasing order to handle tokens with same prefix 
        annot_file = paper[:-4] + 'annot'
        try:
            with open(annot_file) as f:
                for line in f:
                    line = line.strip()
                    if line=='':
                        continue
                    token, label = line.split('##')

                    labels.append([token, self.mapper[label.lower()]])
        except:
            labels = []
        # print(labels)
        labels.sort(key = lambda s: len(s[0]), reverse=True)

    def tagLabels(self, labels, lines, fout):
        number_of_lines = len(lines)
        begin_index = int(self.add_id) + int(self.add_section)
        for line_ind in range(number_of_lines):
            line = lines[line_ind]

            #get only the tokens
            line = [x[0] for x in line]

            #first token of each line is the section. ignore that
            #second token is the document id. ignore that
            i = begin_index
            while i < len(line):
                line_suffix = ' '.join(line[i:])
                for token ,label in labels:
                    #print(temp,token)
                    # print(line_suffix)
                    # if line_suffix.startswith('WIEN2K'):
                    #     print(token, label,line_suffix)

                    #Check if current line contains any labelled token
                    # if line_suffix == token or line_suffix.startswith(token+' '):# or (i==len(line)-1 and line_suffix.startswith(token)):
                        # print('Entered')
                    # print(token)
                    match_obj = re.search(token, line_suffix, re.UNICODE)

                    # if "ZnMnTe and CdMnTe diluted magnetic semiconductors" in line_suffix:
                    #     print(match_obj, token, line_suffix)

                    if match_obj:
                        # print(line_suffix, token)
                        #token_list = token.split();
                        first_part = match_obj.group(1)
                        second_part = match_obj.group(2)
                        matched_part = first_part + second_part
                        token_list = matched_part.split();
                        if label == 'PARAMETER' and len(token_list)==1 and first_part.isdecimal() and token_list[0]!=first_part:
                            # print(token_list, first_part)
                            pass
                        else:
                            # print(token_list)
                            lines[line_ind][i][1] = 'B-'+label
                            for k in range(1,len(token_list)):
                                lines[line_ind][i+k][1] = 'I-'+label
                            i = i+len(token_list)-1;
                            break
                i+=1

        for line in lines:
            labels_list = set([each[1] for each in line[begin_index:]])
            # print(labels_list)
            if not self.all_lines and len(labels_list)<=1:
                continue

            # num = random.random()
            # if num>0.5:
            #     continue

            #fout.write(line[0] + '\n')
            for each in line:
                fout.write(str(each[0] + ' ' + each[1] + '\n'))
            fout.write(str('\n'))


def str2bool(v):
    if isinstance(v, bool):
      return v
    if v.lower() in ('yes', 'true', 't', 'y', '1'):
        return True
    elif v.lower() in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        return False

#paper_dir = '/home/souradip-pg/MtechProject/pdfs/updated_annotations/output_pdfannots/'
# single_annotation = re.compile(r'(.*?)<<<<(.*?)>>>>##\[(.*?)\](.*?)')
# sequence_annotation_start = re.compile(r'(.*)<<<<(.*)')
# sequence_annotation_end = re.compile(r'(.*)>>>>##\[(.*)\](.*)')

# replace = re.compile(r'<<<<(.*?)>>>>##\[(.*?)\]')
# braces = re.compile(r'~(.*?)!')
# charsub_empty = re.compile(r"['\(\);]")
# charsub_space = re.compile(r"([,:])")

# mapper = {'': 'INVALID', 'result': 'RESULT', 'method':'METHOD',
# 'parameters':'PARAMETER', 'parameter':'PARAMETER','material':'MATERIAL',
# 'meterial':'MATERIAL', 'materials':'MATERIAL', 'xc':'METHOD', 'code':'CODE', 'structure':'STRUCTURE'}

if __name__== "__main__" :

    parser = argparse.ArgumentParser(description='Create dataset.')
    parser.add_argument("-inputDirectory",dest='inputDirectory',action='store',help="directory that contains the parsed pdf files")
    parser.add_argument("-outputFile",dest='outputFile',action='store', help="name of the output file")
    parser.add_argument("-addSection",dest='addSection',action='store', help="add section information")
    parser.add_argument("-addId",dest='addId',action='store', help="add id information")
    parser.add_argument("-allLines",dest='allLines',action='store', help="add all lines")
    args = parser.parse_args()
    # args = parser.parse_args(['-inputDirectory','/content/drive/My Drive/Project/dataset/pdfs/data_09-12-2019/one/dev', 
    #                           '-outputFile','data_09-12-2019/one/dev.txt',
    #                           '-addSection','true',
    #                           '-addId','true',
    #                           '-allLines','true'])

    output_file = args.outputFile
    paper_dir = args.inputDirectory
    add_section = str2bool(args.addSection)
    add_id = str2bool(args.addId)
    all_lines = str2bool(args.allLines)

    print("Paper directory - " + paper_dir)
    print("Output file - " + output_file)
    print("Add section - " + str(add_section))
    print("Add id - " + str(add_id))
    print("All lines - " + str(all_lines))

    #nlp = en_core_sci_sm.load()
    #nlp.tokenizer = Tokenizer(nlp.vocab)

    createDataset = CreateDataset(add_id, add_section, all_lines)

    label_dict = {}
    cnt = 0
    with io.open(output_file, mode='w', encoding='utf8') as fout:
        for subdir, dirs, files in os.walk(paper_dir):
            for file in tqdm(files, desc="Files"):
                labels = []
                lines = []

                paper = os.path.join(subdir, file)
                if not paper.endswith('.json'):
                    continue
                # print(paper)
                # if paper != '../../pdfs/data_18-01-2020/one/test/26-08-2019-paper-2.pdf.json':
                #     continue
                # createDataset.processFile(paper+'.updated', labels, lines)
                createDataset.processFile(paper, labels, lines)

                # annot_file = paper[:-4] + 'annot'
                # try:
                #     with open(annot_file) as f:
                #         for line in f:
                #             line = line.strip()
                #             if line=='':
                #                 continue
                #             token, label = line.split('##')

                #             labels.append([token, createDataset.mapper[label.lower()]])

                #     # print(paper, labels)
                createDataset.tagLabels(labels, lines, fout)
                #     cnt += 1
                # except:
                #     print('error    ')

#!python3 create_dataset.py -inputDirectory '/content/drive/My Drive/Project/dataset/pdfs/data_09-12-2019/one/dev' -outputFile 'data_09-12-2019/one/dev.txt' -addSection 'true' -addId 'true'
